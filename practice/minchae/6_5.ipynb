{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dba4c8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da59a4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5965, 0.6040, 0.8874],\n",
      "        [0.4241, 0.7518, 0.4177],\n",
      "        [0.5110, 0.9830, 0.5830]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "target = torch.linspace(0.1, 0.9, steps=9).reshape(3, 3)\n",
    "x = torch.rand_like(target)\n",
    "x.requires_grad = True # x에 대해 미분을 수행할 수 있도록 설정\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b88860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1135, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 초기 loss 계산: MSE 사용\n",
    "\n",
    "loss = F.mse_loss(x, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f30367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-th Loss: 6.8677e-02\n",
      "tensor([[0.4862, 0.5142, 0.7569],\n",
      "        [0.4188, 0.6959, 0.4582],\n",
      "        [0.5530, 0.9423, 0.6535]], requires_grad=True)\n",
      "2-th Loss: 4.1545e-02\n",
      "tensor([[0.4004, 0.4444, 0.6553],\n",
      "        [0.4146, 0.6523, 0.4897],\n",
      "        [0.5857, 0.9107, 0.7082]], requires_grad=True)\n",
      "3-th Loss: 2.5132e-02\n",
      "tensor([[0.3336, 0.3901, 0.5764],\n",
      "        [0.4114, 0.6185, 0.5142],\n",
      "        [0.6111, 0.8861, 0.7509]], requires_grad=True)\n",
      "4-th Loss: 1.5203e-02\n",
      "tensor([[0.2817, 0.3479, 0.5150],\n",
      "        [0.4088, 0.5922, 0.5333],\n",
      "        [0.6308, 0.8670, 0.7840]], requires_grad=True)\n",
      "5-th Loss: 9.1971e-03\n",
      "tensor([[0.2413, 0.3150, 0.4672],\n",
      "        [0.4069, 0.5717, 0.5481],\n",
      "        [0.6462, 0.8521, 0.8098]], requires_grad=True)\n",
      "6-th Loss: 5.5637e-03\n",
      "tensor([[0.2099, 0.2894, 0.4300],\n",
      "        [0.4053, 0.5557, 0.5596],\n",
      "        [0.6582, 0.8405, 0.8298]], requires_grad=True)\n",
      "7-th Loss: 3.3657e-03\n",
      "tensor([[0.1855, 0.2696, 0.4011],\n",
      "        [0.4042, 0.5434, 0.5686],\n",
      "        [0.6675, 0.8315, 0.8454]], requires_grad=True)\n",
      "8-th Loss: 2.0360e-03\n",
      "tensor([[0.1665, 0.2541, 0.3787],\n",
      "        [0.4032, 0.5337, 0.5756],\n",
      "        [0.6747, 0.8245, 0.8575]], requires_grad=True)\n",
      "9-th Loss: 1.2317e-03\n",
      "tensor([[0.1517, 0.2421, 0.3612],\n",
      "        [0.4025, 0.5262, 0.5810],\n",
      "        [0.6803, 0.8191, 0.8670]], requires_grad=True)\n",
      "10-th Loss: 7.4509e-04\n",
      "tensor([[0.1402, 0.2327, 0.3476],\n",
      "        [0.4020, 0.5204, 0.5852],\n",
      "        [0.6847, 0.8148, 0.8743]], requires_grad=True)\n",
      "11-th Loss: 4.5073e-04\n",
      "tensor([[0.1313, 0.2255, 0.3370],\n",
      "        [0.4015, 0.5159, 0.5885],\n",
      "        [0.6881, 0.8115, 0.8800]], requires_grad=True)\n",
      "12-th Loss: 2.7267e-04\n",
      "tensor([[0.1243, 0.2198, 0.3288],\n",
      "        [0.4012, 0.5123, 0.5911],\n",
      "        [0.6907, 0.8090, 0.8845]], requires_grad=True)\n",
      "13-th Loss: 1.6495e-04\n",
      "tensor([[0.1189, 0.2154, 0.3224],\n",
      "        [0.4009, 0.5096, 0.5931],\n",
      "        [0.6928, 0.8070, 0.8879]], requires_grad=True)\n",
      "14-th Loss: 9.9782e-05\n",
      "tensor([[0.1147, 0.2120, 0.3174],\n",
      "        [0.4007, 0.5075, 0.5946],\n",
      "        [0.6944, 0.8054, 0.8906]], requires_grad=True)\n",
      "15-th Loss: 6.0362e-05\n",
      "tensor([[0.1114, 0.2093, 0.3135],\n",
      "        [0.4006, 0.5058, 0.5958],\n",
      "        [0.6956, 0.8042, 0.8927]], requires_grad=True)\n",
      "16-th Loss: 3.6515e-05\n",
      "tensor([[0.1089, 0.2072, 0.3105],\n",
      "        [0.4004, 0.5045, 0.5967],\n",
      "        [0.6966, 0.8033, 0.8943]], requires_grad=True)\n",
      "17-th Loss: 2.2090e-05\n",
      "tensor([[0.1069, 0.2056, 0.3082],\n",
      "        [0.4003, 0.5035, 0.5975],\n",
      "        [0.6974, 0.8026, 0.8956]], requires_grad=True)\n",
      "18-th Loss: 1.3363e-05\n",
      "tensor([[0.1054, 0.2044, 0.3064],\n",
      "        [0.4003, 0.5027, 0.5980],\n",
      "        [0.6979, 0.8020, 0.8966]], requires_grad=True)\n",
      "19-th Loss: 8.0837e-06\n",
      "tensor([[0.1042, 0.2034, 0.3050],\n",
      "        [0.4002, 0.5021, 0.5985],\n",
      "        [0.6984, 0.8015, 0.8973]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 학습 파라미터 설정\n",
    "threshold = 1e-5    # loss가 이 값 이하로 떨어지면 학습 중단\n",
    "lr = 1.0            # 학습률(Learning Rate)\n",
    "iter_cnt = 0        # 반복 횟수 카운트\n",
    "\n",
    "# loss가 threshold보다 클 동안 반복 학습\n",
    "while loss > threshold:\n",
    "    iter_cnt += 1\n",
    "    loss.backward()                 # 손실 함수에 대해 x를 기준으로 미분\n",
    "\n",
    "    x = x - lr * x.grad             # 경사 하강법으로 파라미터 업데이트\n",
    "    x.detach_()                     # 그래프 끊기 (이전 그래프와 분리)\n",
    "    x.requires_grad_(True)          # 새로 업데이트된 x에 대해 다시 미분 가능 설정\n",
    "\n",
    "    loss = F.mse_loss(x, target)    # 업데이트된 x로 새로운 loss 계산\n",
    "\n",
    "    # 반복 횟수, 손실 값, x 출력\n",
    "    print('%d-th Loss: %.4e' % (iter_cnt, loss))\n",
    "    print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
